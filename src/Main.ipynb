{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5bc6ddc7638f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'manager'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import skimage.io\n",
    "import glob\n",
    "import os\n",
    "import functools\n",
    "from albumentations import *\n",
    "from sklearn import model_selection, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from config import Config\n",
    "from manager import DataManager\n",
    "from util import get_optimizer\n",
    "from generator import get_dataset\n",
    "from model import NeuralNet, fit, predict\n",
    "\n",
    "\n",
    "#tf.config.set_visible_devices([], 'GPU')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = DataManager.get_train_file()\n",
    "\n",
    "# dataset = get_dataset(\n",
    "#     dataframe=data, \n",
    "#     input_path=Config.input.path,\n",
    "#     batch_size=Config.train.batch_size,\n",
    "#     training=True,\n",
    "#     augment=True,\n",
    "#     buffer_size=1,\n",
    "#     cache=False,\n",
    "# )\n",
    "\n",
    "# for x,y in dataset.take(1):\n",
    "#     x = x.numpy()\n",
    "#     print(\"x shape =\", x.shape)\n",
    "\n",
    "# fig, axes = plt.subplots(1, Config.train.batch_size, figsize=(20, 20))\n",
    "\n",
    "# if Config.train.batch_size > 1:\n",
    "#     for i, ax in enumerate(axes.reshape(-1)):\n",
    "#         ax.imshow(x[i])\n",
    "#         ax.axis('off')\n",
    "# else:\n",
    "#     axes.imshow(x[0])\n",
    "#     axes.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-26853b5e8bae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m optimizer = get_optimizer(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mlr_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = Config.input.input_shape\n",
    "input_path = Config.input.path\n",
    "seed = Config.train.seed\n",
    "folds = Config.train.folds\n",
    "batch_size = Config.train.batch_size\n",
    "epochs = Config.train.epochs\n",
    "accum_steps = Config.train.accum_steps\n",
    "\n",
    "lr_steps_per_epoch=math.ceil((10616 * (1-1/Config.train.folds)) / Config.train.batch_size)\n",
    "lr_max=Config.train.learning_rate.max\n",
    "lr_min=Config.train.learning_rate.min\n",
    "lr_decay_epochs=Config.train.learning_rate.decay_epochs\n",
    "lr_warmup_epochs=Config.train.learning_rate.warmup_epochs\n",
    "lr_power=Config.train.learning_rate.power\n",
    "\n",
    "units=Config.model.units\n",
    "dropout=Config.model.dropout\n",
    "activation=Config.model.activation\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = get_optimizer(\n",
    "    steps_per_epoch=lr_steps_per_epoch,\n",
    "    lr_max=lr_max,\n",
    "    lr_min=lr_min,\n",
    "    decay_epochs=lr_decay_epochs,\n",
    "    warmup_epochs=lr_warmup_epochs,\n",
    "    power=lr_power\n",
    ")\n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer, loss_scale='dynamic')\n",
    "\n",
    "classes = np.where(data.data_provider == 'karolinska', 6, 0) + data.isup_grade.values\n",
    "skf = model_selection.StratifiedKFold(\n",
    "    folds, shuffle=True, random_state=seed).split(data.image_id, y=data.isup_grade)\n",
    "\n",
    "for fold_num, (train_idx, valid_idx) in enumerate(skf):\n",
    "\n",
    "    if fold_num == 0:\n",
    "        model = NeuralNet(\n",
    "            engine=EfficientNetB0, \n",
    "            input_shape=input_shape, \n",
    "            units=units,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            weights='noisy-student')\n",
    "        model.build([None, *input_shape])\n",
    "        \n",
    "        train_dataset = get_dataset(\n",
    "            dataframe=data.iloc[train_idx], \n",
    "            input_path=input_path, \n",
    "            batch_size=batch_size,\n",
    "            training=True,\n",
    "            augment=True,\n",
    "            buffer_size=1024,\n",
    "            cache=False,\n",
    "        )\n",
    "        \n",
    "        valid_dataset = get_dataset(\n",
    "            dataframe=data.iloc[valid_idx], \n",
    "            input_path=input_path, \n",
    "            batch_size=batch_size,\n",
    "            training=False,\n",
    "            augment=False,\n",
    "            buffer_size=1,\n",
    "            cache=True,\n",
    "        )\n",
    "\n",
    "        best_score = float('-inf')\n",
    "        for epoch_num in range(epochs):\n",
    "            \n",
    "            fit(model, train_dataset, loss_fn, optimizer, accum_steps)\n",
    "\n",
    "            preds, trues = predict(model, valid_dataset)\n",
    "\n",
    "            if Config.infer.tta > 1:\n",
    "                preds = preds.reshape((-1, Config.infer.tta)).mean(-1)\n",
    "                trues = trues.reshape((-1, Config.infer.tta)).mean(-1)\n",
    "                \n",
    "            preds = np.round(preds, 0)\n",
    "            trues = np.round(trues, 0)\n",
    "            score = metrics.cohen_kappa_score(trues, preds, weights='quadratic')\n",
    "            print(\"\\nscore =\", score)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                model.save_weights(f'output/weights/model-{fold_num}-{epoch_num}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
